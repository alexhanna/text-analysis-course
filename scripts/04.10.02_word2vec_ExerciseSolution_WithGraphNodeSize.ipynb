{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec on the Akkadian ORACC corpus\n",
    "\n",
    "This lesson is designed to explore features of word embeddings produced through the word2vec model.\n",
    "\n",
    "The primary corpus we use consists of [Akkadian ORACC corpus](https://github.com/niekveldhuis/Word2vec), put together by Professor Niek Veldhuis, UC Berkeley Near Eastern Studies.\n",
    "\n",
    "At then end we'll also look at a <a href=\"http://ryanheuser.org/word-vectors-1/\">Word2Vec model trained on the ECCO-TCP corpus</a> of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that I have shortened the number of terms in the model by half in order to conserve memory.)\n",
    "\n",
    "### Learning Goals\n",
    "* Learn the intuition behind word embedding models (WEM)\n",
    "* Learn how to implement a WEM using the gensim implementation of word2vec\n",
    "* Explore a completely unknown corpus using this method (unknown to most of you)\n",
    "* Think through how visualization of WEM might help you explore your corpus\n",
    "* Implement text analysis on a non-English language\n",
    "\n",
    "### Agenda\n",
    "<ol>\n",
    "<li>Import & Pre-Processing</li>\n",
    "<li>Word2Vec</li>\n",
    "<ol><li>Training</li>\n",
    "<li>Embeddings</li>\n",
    "<li>Visualization</li>\n",
    "</ol>\n",
    "<li>Saving/Loading Models</li>\n",
    "</ol>\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "For further background on Word2Vec's mechanics, I suggest this <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html\">brief tutorial</a> by Google, especially the sections \"Motivation,\" \"Skip-Gram Model,\" and \"Visualizing.\"\n",
    "\n",
    "Ben Schmidt's blogs [here](http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html) and [here](http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prep\n",
    "\n",
    "Install a new package, and import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Install a package that is not in the Anaconda distribution\n",
    "#To do this we'll use pip install\n",
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "\n",
    "#Data Wrangling\n",
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import gensim #library needed for word2vec\n",
    "\n",
    "#for visualization\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.manifold import MDS, TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualization parameters\n",
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and Pre-Processing\n",
    "\n",
    "### Corpus Description\n",
    "\n",
    "The corpus description can be found [here](https://github.com/niekveldhuis/Word2vec).\n",
    "\n",
    "### Import Data\n",
    "\n",
    "Read in all of the .csv files in the folder `../data/oracc/`, do some pre-processing on it, and concat them all into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in all the data, with some cleaning\n",
    "#I won't explain this code, but challenge yourself to understand it\n",
    "path ='../data/oracc/' # indicate the local path where files are stored\n",
    "allFiles = os.listdir(path) #save the list of filenames into a variable\n",
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_ = []\n",
    "files_ = []\n",
    "for file_ in allFiles:\n",
    "    filename = path+file_ #add the relative path name to the filename\n",
    "    df = pandas.read_csv(filename,index_col=None, header=0)\n",
    "    df['id_text'] = [file_[7:-4].replace('_', '/') + '/' + text for text in df['id_text']]\n",
    "    df['lemma'] = [lemma.replace('$', '') for lemma in df['lemma']]\n",
    "    list_.append(df)\n",
    "    files_.append(file_[7:-4].replace('_', '/'))\n",
    "data = pandas.concat(list_).reset_index(drop=True)\n",
    "#view the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of rows\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#View the first text\n",
    "data.iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "Word2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. In this text there is no punctuation, and thus nothing resembling a sentence. In other text we  want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\n",
    "\n",
    "You can split your text in sentences using ` nltk.tokenize.sent_tokenize()`\n",
    "\n",
    "For today, we'll tokenize our text by spliting on the white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenize the data by splitting on white space. There is no punctuation in this text.\n",
    "data['tokens'] = data['lemma'].str.split()\n",
    "data['tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Unlemmatized (broken or unknown) words are represented as, for instance, `x-ši-ka[NA]NA`. Such tokens are essentially placeholders. One may try two different approaches:\n",
    "- represent all such placeholders by NA\n",
    "- eliminate all placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_NA = data.copy()\n",
    "data_NA['tokens'] = data_NA['tokens'].apply(lambda x: [token if not token.endswith('NA]NA') else 'NA' for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(lambda x: [token for token in x if not token.endswith('NA]NA')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_NA['tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec\n",
    "\n",
    "### Word Embedding\n",
    "Word2Vec is the most prominent word embedding algorithm. Word embedding generally attempts to identify semantic relationships between words by observing them in context.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of Word2Vec are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. Skip-Gram takes a word of interest as its input (e.g. \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\"). CBOW does the opposite, taking the context words (\"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Size: Number of dimensions for word embedding model</li>\n",
    "<li>Window: Number of context words to observe in each direction</li>\n",
    "<li>min_count: Minimum frequency for words included in model</li>\n",
    "<li>sg (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram</li>\n",
    "<li>Alpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Iterations: Number of passes through dataset</li>\n",
    "<li>Batch Size: Number of words to sample from data during each pass</li>\n",
    "<li>Worker: Set the 'worker' option to ensure reproducibility</li>\n",
    "</ul>\n",
    "\n",
    "Note: Script uses default value for each argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, or fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(data['tokens'], size=100, window=5, \\\n",
    "                               min_count=1, sg=1, alpha=0.025, iter=5, batch_words=10000, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return dense word vector for the word 'ēkallu[palace]N'\n",
    "#each token (not document) has a 100 element vector\n",
    "model['ēkallu[palace]N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-Space Operations\n",
    "\n",
    "#### Similarity\n",
    "Since words are represented as dense vectors, we can ask how similiar words' meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few dout-of-the-box functions that enable different kinds of comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cosine distance between two given word vectors\n",
    "model.similarity('ēkallu[palace]N','bītu[house]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find the 10 most similar vectors to the given word vector, using cosine distance\n",
    "model.most_similar('ēkallu[palace]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('immeru[sheep]N','puhādu[lamb]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('arhu[cow]N','būru[(bull)-calf]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##EX: find the most similar words to cow and sheep. Do they make sense?\n",
    "model.most_similar('arhu[cow]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('immeru[sheep]N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Valences\n",
    "\n",
    "A word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of <em>river bank</em> from the word <em>bank</em>. This would be written mathetmatically as <em>RIVER - BANK</em>, which in <em>gensim</em>'s interface lists <em>RIVER</em> as a positive meaning and <em>BANK</em> as a negative one.\n",
    "\n",
    "We'll try to find different meanings of the words 'bad' and 'good' in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(['masku[bad]AJ','lemnu[bad]AJ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to capture 'bad' in the magical, sorcery sense, and perhaps as injustice. Let's remove those vectors from the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove more vectors to get at different senses of the word 'bad'\n",
    "model.most_similar(positive=['masku[bad]AJ','lemnu[bad]AJ'], negative=['utukku[(an-evil-demon)]N','dipalû[distortion-of-justice]N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets at a slightly different sense of the word 'bad', relating to battle and military force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. Use the most_similar method to find the tokens nearest to 'good' in our model.\n",
    "##The strings for good are 'damqu[good]AJ' and 'ṭābu[good]AJ'.\n",
    "print(model.most_similar(['damqu[good]AJ', 'ṭābu[good]AJ']))\n",
    "print()\n",
    "## EX. Remove the vector 'hadû[joyful]AJ' from the 'good' vector.\n",
    "## What alternative meaning of 'good' comes through?\n",
    "print(model.most_similar(positive=['damqu[good]AJ', 'ṭābu[good]AJ'], negative=['hadû[joyful]AJ']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy\n",
    "Analogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy <em>MAN is to KING as WOMAN is to ??</em> is rendered as <em>KING - MAN + WOMAN</em>. In the gensim interface, we designate <em>KING</em> and <em>WOMAN</em> as positive terms and <em>MAN</em> as a negative term, since it is subtracted from those.\n",
    "\n",
    "We'll try this with the analogy Cow::Calf as Sheep::?? (the word we are looking for is lamb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['immeru[sheep]N', 'būru[(bull)-calf]N'], negative=['arhu[cow]N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a binary: Horses and Sheep\n",
    "\n",
    "Ben Schimdt found the meat/vegetable binary as a useful binary to see in a vector space. We can find an analogous binary here.\n",
    "\n",
    "The animal vocabulary may be divided into 'horse-vocabulary' (used for war and often received from foreign countries) and sheep vocabulary. Sheep are domestic animals held for meat and wool and are (relatively) close to other such animals (ox, calf) and words that have to do with wool production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "animals = ['sisû[horse]N', 'immeru[sheep]N', 'imēru[donkey]N', 'alpu[ox]N', 'littu[cow]N', \n",
    "           'pīru[elephant]N', 'yābilu[ram]N', 'udru[Bactrian-camel]N', 'damdāmu[(a-kind-of-mule)]N'\n",
    "           ,'atānu[she-ass]N', 'būru[(bull)-calf]N', 'tuānu[(a-breed-of-horse)]N', 'agālu[donkey]N'\n",
    "          , 'šullāmu[(a-type-of-horse)]N', 'sugullu[herd]N', 'naṣmadu[harness]N', 'ṣamādu[team]N'\n",
    "          ,'harbu[plough]N', 'Parsuaya[from-Parsua]EN', 'šulušīu[three-year-old]AJ', 'kīṣu[flayed]AJ'\n",
    "          ,'bitrumu[very-colourful]AJ', 'buqūmu[plucking]N', 'anāqāte[she-camels]N',\n",
    "           'udukiutukku[(a-kind-of-sacrificial-sheep)]N', 'maḫirtu[(a-bone-of-the-leg)]N', 'Muṣuraya[Egyptian]EN',\n",
    "          'gurrutu[ewe]N', 'irginu[(a-breed-or-colour-of-horse)]N', 'ṣummudu[equipped]AJ', 'qummānu[(a-sheep)]N',\n",
    "           'baqmu[plucked]AJ', 'huzīru[pig]N', 'surrudu[packed-up]AJ', 'pēthallu[riding-horse]N', 'nāmurtu[audience-gift]N', \n",
    "           'Manna[Mannea]GN', 'puhādu[lamb]N']\n",
    "animal_words = model.most_similar(animals, topn=100)\n",
    "animal_words = [word for word, similarity in animal_words]\n",
    "animal_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize this 'sheep 'horse' binary by plotting the vector space for these two words on the same graph. This is similar to the 'meat' 'vegetable' binary graphed by Ben Schmidt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [model.similarity('sisû[horse]N', word) for word in animals]\n",
    "y = [model.similarity('immeru[sheep]N', word) for word in animals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an array with relative count frequencies for each word to scale the size of each node based on the relative frequency in the text.\n",
    "\n",
    "Thanks to classmate Richard Doan for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a count dictionary\n",
    "counts = {}\n",
    "for sentence in data['tokens']:\n",
    "    for word in sentence:\n",
    "        if word not in counts:\n",
    "            counts[word] = 0\n",
    "        counts[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creat an array for the size, based on the relative count\n",
    "sizes = []\n",
    "for animal in animals:\n",
    "    sizes.append(counts[animal])\n",
    "\n",
    "sizes = list(map(lambda x: x / max(sizes), sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc('font', family='Arial')\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20,20))\n",
    "ax.scatter(x, y, sizes, alpha=1, color='b')\n",
    "for i in range(len(animals)):\n",
    "    ax.annotate(animals[i], (x[i], y[i]))\n",
    "ax.set_xlim(.25, 1.1)\n",
    "ax.set_ylim(.4, 1.1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q. What kinds of semantic relationships exist in the diagram above?\n",
    "####    Are there any words that seem out of place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving/Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save current model for later use\n",
    "\n",
    "model.wv.save_word2vec_format('../data/word2vec.oracc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load up models from disk\n",
    "\n",
    "# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n",
    "# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n",
    "\n",
    "ecco_model = gensim.models.Word2Vec.load_word2vec_format('../data/word2vec.ECCO-TCP.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can we get the currency sense of the word bank in Ryan Heuser's model?\n",
    "\n",
    "ecco_model.most_similar(positive=['bank'], negative=['river'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n",
    "##     RICHES are to VIRTUE what LEARNING is to GENIUS.\n",
    "## Reproduce this analogy using his trained word2vec model\n",
    "\n",
    "##  Q. How might we compare word2vec models more generally?\n",
    "ecco_model.most_similar(positive=['virtue', 'learning'], negative=['riches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Open Questions\n",
    "At this point, we have seen a number of mathemetical operations that we may use to explore word2vec's word embeddings. These enable us to answer a set of new, interesting questions dealing with semantics, yet there are many other questions that remain unanswered.\n",
    "\n",
    "For example:\n",
    "<ol>\n",
    "<li>How to compare word usages in different texts (within the same model)?</li>\n",
    "<li>How to compare word meanings in different models? compare whole models?</li>\n",
    "<li>What about the space “in between” words?</li>\n",
    "<li>Do we agree with the Distributional Hypothesis that words with the same contexts share their meanings?</li>\n",
    "<ol><li>If not, then what information do we think is encoded in a word’s context?</li></ol>\n",
    "<li>What good, humanistic research questions do analogies shed light on?</li>\n",
    "<ol><li>shades of meaning?</li><li>context similarity?</li></ol>\n",
    "</ol>\n",
    "\n",
    "With the time remaining, play around with either of these two word2vec models, or begin to implement it on your own corpus."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
