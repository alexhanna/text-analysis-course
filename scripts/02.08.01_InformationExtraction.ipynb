{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging and Named Entity Recognition using NLTK\n",
    "\n",
    "One task in NLP has been to reliably identify a word's part of speech. This can help us with the ever-present task of identifying content words, but can be used in a variety of analyses. Part-of-speech tagging is a specific instance in the larger category of word tagging, or placing words in pre-determined categories.\n",
    "\n",
    "Another instance of word tagging named entity recognition. \n",
    "\n",
    "Today we'll learn how to identify a word's part of speech and how to extract named entities using NLTK, and think through reasons we may want to do this..\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "* Understand the intuition behind tagging and information extraction\n",
    "* Use NLTK to tag the part of speech of each word\n",
    "* Count most frequent words based on their part of speech\n",
    "* Extract named entities from a text, and in doing so, learn more about chunking text\n",
    "* Be equipped to explore this further to get better accuracy for your chosen corpus\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "* *part-of-speech tagging*: \n",
    "    * the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context\n",
    "* *named entity recognition*:\n",
    "    * a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc\n",
    "* *tree* \n",
    "    * data structure made up of nodes or vertices and edges without having any cycle. \n",
    "* *treebank*:\n",
    "    * a parsed text corpus that annotates syntactic or semantic sentence structure\n",
    "* *tuple*:\n",
    "    * a sequence of immutable Python objects\n",
    "\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "For more information on information extraction using NLTK, see chapter 7: http://www.nltk.org/book/ch07.html\n",
    "\n",
    "### 0. Part-of-Speech Tagging\n",
    "On Monday you may have noticed that stop words are typically short function words. Intuitively, if we could identify the part of speech of a word, we would have another way of identifying content words. NLTK can do that too!\n",
    "\n",
    "NLTK has a function that will tag the part of speech of every token in a text. For this, we will re-create our original tokenized text sentence from Monday, with the stop words and punctuation.\n",
    "\n",
    "NLTK uses the Penn Treebank Project to tag the part-of-speech of the words. The NLTK algoritm is deterministic - it assigns the most common part of speech for each word, as found in the Penn Treebank. You can find a list of all the part-of-speech tags here:\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For',\n",
       " 'me',\n",
       " 'it',\n",
       " 'has',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'the',\n",
       " 'work',\n",
       " 'that',\n",
       " 'gets',\n",
       " 'done',\n",
       " 'at',\n",
       " 'the',\n",
       " 'crossroads',\n",
       " 'of',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'and',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'study',\n",
       " '.',\n",
       " 'And',\n",
       " 'that',\n",
       " 'happens',\n",
       " 'in',\n",
       " 'two',\n",
       " 'different',\n",
       " 'ways',\n",
       " '.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'one',\n",
       " 'hand',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'bringing',\n",
       " 'the',\n",
       " 'tools',\n",
       " 'and',\n",
       " 'techniques',\n",
       " 'of',\n",
       " 'digital',\n",
       " 'media',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'on',\n",
       " 'traditional',\n",
       " 'humanistic',\n",
       " 'questions',\n",
       " ';',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'also',\n",
       " 'bringing',\n",
       " 'humanistic',\n",
       " 'modes',\n",
       " 'of',\n",
       " 'inquiry',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'on',\n",
       " 'digital',\n",
       " 'media',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"For me it has to do with the work that gets done at the crossroads of \\\n",
    "digital media and traditional humanistic study. And that happens in two different ways. \\\n",
    "On the one hand, it's bringing the tools and techniques of digital media to bear \\\n",
    "on traditional humanistic questions; on the other, it's also bringing humanistic modes \\\n",
    "of inquiry to bear on digital media.\"\n",
    "\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "#check we did everything correctly\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 'IN'),\n",
       " ('me', 'PRP'),\n",
       " ('it', 'PRP'),\n",
       " ('has', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('do', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('work', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('gets', 'VBZ'),\n",
       " ('done', 'VBN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('crossroads', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('digital', 'JJ'),\n",
       " ('media', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('traditional', 'JJ'),\n",
       " ('humanistic', 'JJ'),\n",
       " ('study', 'NN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('that', 'DT'),\n",
       " ('happens', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('two', 'CD'),\n",
       " ('different', 'JJ'),\n",
       " ('ways', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('On', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('one', 'CD'),\n",
       " ('hand', 'NN'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('bringing', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('tools', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('techniques', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('digital', 'JJ'),\n",
       " ('media', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('bear', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('traditional', 'JJ'),\n",
       " ('humanistic', 'JJ'),\n",
       " ('questions', 'NNS'),\n",
       " (';', ':'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('also', 'RB'),\n",
       " ('bringing', 'VBG'),\n",
       " ('humanistic', 'JJ'),\n",
       " ('modes', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('inquiry', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('bear', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('digital', 'JJ'),\n",
       " ('media', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the nltk pos function to tag the tokens\n",
    "tagged_sentence_tokens = nltk.pos_tag(sentence_tokens)\n",
    "\n",
    "#view tagged sentence\n",
    "tagged_sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes more complicated code. Stay with me. The above output is a list of *tuples*. A tuple is a sequence of Python objects. In this case, each of these tuples is a sequence of strings. To loop through tuples is intuitively the same as looping through a list, but slightly different syntax. \n",
    "\n",
    "Note that this is not a list of lists, as we saw in our lesson on Pandas. This is a list of tuples.\n",
    "\n",
    "Let's pull out the part-of-speech tag from each tuple above and save that to a list. Notice the order stays exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IN', 'PRP', 'PRP', 'VBZ', 'TO', 'VB', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'VBN', 'IN', 'DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'JJ', 'JJ', 'NN', '.', 'CC', 'DT', 'VBZ', 'IN', 'CD', 'JJ', 'NNS', '.', 'IN', 'DT', 'CD', 'NN', ',', 'PRP', 'VBZ', 'VBG', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'IN', 'JJ', 'JJ', 'NNS', ':', 'IN', 'DT', 'JJ', ',', 'PRP', 'VBZ', 'RB', 'VBG', 'JJ', 'NNS', 'IN', 'NN', 'TO', 'VB', 'IN', 'JJ', 'NNS', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tags = [tag for (word, tag) in tagged_sentence_tokens]\n",
    "print(word_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the difference in syntax for the above code compared to our standard list comprehension code?\n",
    "\n",
    "\n",
    "We can count the part-of-speech tags in a similar way we counted words, to output the most frequent types of words in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IN', 11),\n",
       " ('JJ', 10),\n",
       " ('NNS', 9),\n",
       " ('DT', 6),\n",
       " ('VBZ', 5),\n",
       " ('PRP', 4),\n",
       " ('NN', 4),\n",
       " ('.', 3),\n",
       " ('CC', 3),\n",
       " ('VB', 3),\n",
       " ('TO', 3),\n",
       " ('VBG', 2),\n",
       " (',', 2),\n",
       " ('CD', 2),\n",
       " ('WDT', 1),\n",
       " (':', 1),\n",
       " ('VBN', 1),\n",
       " ('RB', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_frequency = nltk.FreqDist(word_tags)\n",
    "tagged_frequency.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence contains a lot of adjectives. So let's first look at the adjectives. Notice the syntax here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['digital', 'traditional', 'humanistic', 'different', 'digital', 'traditional', 'humanistic', 'other', 'humanistic', 'digital']\n"
     ]
    }
   ],
   "source": [
    "adjectives = [word for (word,pos) in tagged_sentence_tokens if pos == 'JJ' or pos=='JJR' or pos=='JJS']\n",
    "\n",
    "#print all of the adjectives\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'crossroads', 'media', 'study', 'ways', 'hand', 'tools', 'techniques', 'media', 'questions', 'modes', 'inquiry', 'media']\n"
     ]
    }
   ],
   "source": [
    "nouns = [word for (word,pos) in tagged_sentence_tokens if pos=='NN' or pos=='NNS']\n",
    "\n",
    "#print all of the nouns\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has', 'do', 'gets', 'done', 'happens', \"'s\", 'bringing', 'bear', \"'s\", 'bringing', 'bear']\n"
     ]
    }
   ],
   "source": [
    "#verbs = [word for (word,pos) in tagged_sentence_tokens if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "verbs = [word for (word,pos) in tagged_sentence_tokens if pos in ['VB', 'VBD','VBG','VBN','VBP','VBZ']]\n",
    "\n",
    "#print all of the verbs\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 2), ('bear', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Ex: Print the most frequent nouns, adjective, and verbs in the sentence\n",
    "######What does this tell us?\n",
    "######Compare this to what we did yesterday with removing stop words.\n",
    "\n",
    "freq_nouns = nltk.FreqDist(nouns)\n",
    "freq_nouns.most_common()\n",
    "\n",
    "freq_verbs = nltk.FreqDist(verbs)\n",
    "freq_verbs.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Named Entity Recognition\n",
    "\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. NLTK has a trained classifier to identify named entities from tagged sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Walter/NNP)\n",
      "  W./NNP\n",
      "  Powell/NNP\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  sociologist/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  ,/,\n",
      "  with/IN\n",
      "  a/DT\n",
      "  primary/JJ\n",
      "  appointment/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION School/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Education/NNP)\n",
      "  and/CC\n",
      "  courtesy/NN\n",
      "  appointments/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Schools/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Business/NNP)\n",
      "  and/CC\n",
      "  (GPE Engineering/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  in/IN\n",
      "  (GPE Public/JJ)\n",
      "  Policy/NNP\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  co-director/NN\n",
      "  of/IN\n",
      "  (PERSON Stanford/NNP)\n",
      "  's/POS\n",
      "  (ORGANIZATION Center/NNP)\n",
      "  on/IN\n",
      "  (GPE Philanthropy/NNP)\n",
      "  and/CC\n",
      "  (PERSON Civil/NNP Society/NNP)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  an/DT\n",
      "  external/JJ\n",
      "  faculty/NN\n",
      "  member/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Santa/NNP Fe/NNP Institute/NNP)\n",
      "  since/IN\n",
      "  2001/CD\n",
      "  ./.\n",
      "  He/PRP\n",
      "  has/VBZ\n",
      "  co-authored/VBN\n",
      "  with/IN\n",
      "  (PERSON John/NNP F./NNP Padgett/NNP)\n",
      "  ,/,\n",
      "  (PERSON Valery/NNP Yakubovich/NNP)\n",
      "  ,/,\n",
      "  Xing/VBG\n",
      "  (GPE Zhong/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (PERSON Stansilav/NNP Shekshnia/NNP)\n",
      "  ,/,\n",
      "  among/IN\n",
      "  others/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Sample sentence with a variety of named entities\n",
    "ne_sentence = \"Walter W. Powell is a sociologist at Stanford University, \\\n",
    "with a primary appointment in the School of Education and \\\n",
    "courtesy appointments in the Schools of Business and Engineering, \\\n",
    "and in Public Policy. \\\n",
    "He is co-director of Stanford's Center on Philanthropy and Civil Society. \\\n",
    "He has been an external faculty member at the Santa Fe Institute \\\n",
    "since 2001. He has co-authored with John F. Padgett, Valery Yakubovich, \\\n",
    "Xing Zhong, and Stansilav Shekshnia, among others.\"\n",
    "\n",
    "#tokenize our sentence\n",
    "ne_sentence_tokens = word_tokenize(ne_sentence)\n",
    "#tag each word with its part of speech\n",
    "ne_tagged_sentence_tokens = nltk.pos_tag(ne_sentence_tokens)\n",
    "\n",
    "#use the named entity chunker to tag named entities\n",
    "chunked = nltk.ne_chunk(ne_tagged_sentence_tokens)\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python uses the tree data structure to represent chunked sentences. To pull out only PERSONs, or only ORGNIZATIONs, when can loop through the *subtrees* in the chunked sentence and use the .label() function to identify the named entities of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tree.subtrees of Tree('S', [Tree('PERSON', [('Walter', 'NNP')]), ('W.', 'NNP'), ('Powell', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('sociologist', 'NN'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), (',', ','), ('with', 'IN'), ('a', 'DT'), ('primary', 'JJ'), ('appointment', 'NN'), ('in', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('School', 'NNP')]), ('of', 'IN'), Tree('ORGANIZATION', [('Education', 'NNP')]), ('and', 'CC'), ('courtesy', 'NN'), ('appointments', 'NNS'), ('in', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('Schools', 'NNP')]), ('of', 'IN'), Tree('ORGANIZATION', [('Business', 'NNP')]), ('and', 'CC'), Tree('GPE', [('Engineering', 'NNP')]), (',', ','), ('and', 'CC'), ('in', 'IN'), Tree('GPE', [('Public', 'JJ')]), ('Policy', 'NNP'), ('.', '.'), ('He', 'PRP'), ('is', 'VBZ'), ('co-director', 'NN'), ('of', 'IN'), Tree('PERSON', [('Stanford', 'NNP')]), (\"'s\", 'POS'), Tree('ORGANIZATION', [('Center', 'NNP')]), ('on', 'IN'), Tree('GPE', [('Philanthropy', 'NNP')]), ('and', 'CC'), Tree('PERSON', [('Civil', 'NNP'), ('Society', 'NNP')]), ('.', '.'), ('He', 'PRP'), ('has', 'VBZ'), ('been', 'VBN'), ('an', 'DT'), ('external', 'JJ'), ('faculty', 'NN'), ('member', 'NN'), ('at', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('Santa', 'NNP'), ('Fe', 'NNP'), ('Institute', 'NNP')]), ('since', 'IN'), ('2001', 'CD'), ('.', '.'), ('He', 'PRP'), ('has', 'VBZ'), ('co-authored', 'VBN'), ('with', 'IN'), Tree('PERSON', [('John', 'NNP'), ('F.', 'NNP'), ('Padgett', 'NNP')]), (',', ','), Tree('PERSON', [('Valery', 'NNP'), ('Yakubovich', 'NNP')]), (',', ','), ('Xing', 'VBG'), Tree('GPE', [('Zhong', 'NNP')]), (',', ','), ('and', 'CC'), Tree('PERSON', [('Stansilav', 'NNP'), ('Shekshnia', 'NNP')]), (',', ','), ('among', 'IN'), ('others', 'NNS'), ('.', '.')])>\n"
     ]
    }
   ],
   "source": [
    "#chunked.subtrees\n",
    "print(chunked.subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('PERSON', [('Walter', 'NNP')]),\n",
       " Tree('PERSON', [('Stanford', 'NNP')]),\n",
       " Tree('PERSON', [('Civil', 'NNP'), ('Society', 'NNP')]),\n",
       " Tree('PERSON', [('John', 'NNP'), ('F.', 'NNP'), ('Padgett', 'NNP')]),\n",
       " Tree('PERSON', [('Valery', 'NNP'), ('Yakubovich', 'NNP')]),\n",
       " Tree('PERSON', [('Stansilav', 'NNP'), ('Shekshnia', 'NNP')])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people =  [n for n in chunked.subtrees() if n.label()==\"PERSON\"]\n",
    "orgs = [n for n in chunked.subtrees() if n.label()==\"ORGANIZATION\"]\n",
    "\n",
    "#print people\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments on this list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Walter', 'NNP')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print organizations\n",
    "len(orgs)\n",
    "set(people[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it's not very accurate. Wilkens used the Stanford NER in his paper, which by many measures in much more accurate. There is a way to call Stanford's tools in Python, but it requires downloading a bunch of things and refering to them in your Python code. I will write a blog post about this for D-Lab, so stay tuned. If you want to use this in the final project you can explore this option with me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Ex: Compare the most frequent part-of-speech used in two of the texts in our data folder.\n",
    "##Ex: Compare the *number* of organizations and people mentioned in two of the texts in our data folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
